\section{Related Work}
There are the large variety of network security devices and mechanisms can secure the network traffic. But these methods are becoming less detection performance since the network attacks evolve much rapidly than the state-of-the-art detection approaches. One of the biggest changes in network security is the using of the HTTP protocol. In the past, the HTTP protocol was usually used to browse the web. But now it is not only used in web browsing but also for other types of uses like the malware attacks.*1

Most of the malware would try to connect with the C\&C server to send the stolen sensitive information or receive commands after infected the victim machine. Due to the HTTP is generally open communication channel in the most network, many malware uses it to communicate with C\&C server. Furthermore, HTTP protocol is heavily used in every machine, malware can easily hide in the traffic to avoid being detected. In order to evade modern detection systems, malware would also fill in the User-Agent which is in the HTTP headers to pretend that they are legitimate HTTP requests.*2

In the past, the system that uses the HTTP headers to detect suspicious traffic is mainly focused on the User-Agent. As Kheir *3 works, they use the general signatures which call of User-Agents to filter out the same signature cluster as labeled as legitimate and the left is labeled as malicious. The biggest problem with this approach is that it can only detect specific malware with use constant User-Agent field to connect or send through HTTP protocol. In addition, this system also needs lots of data to cluster most of the legal User-Agents.

Riccardo et al.*4 pointed out that the two most advanced technologies currently used in security tools, such as signature-based techniques and anomaly-based detection have some problem. The signature-based techniques rely on the known of malware samples, it cannot identify new or unknown malware. And the anomaly-based detection needs lots of malicious data to build the model. However, there is not easy to get the malicious traffic from the variety of malware. Therefore they propose a system, call DECANTeR which uses passive way to extract HTTP headers to application fingerprinting. This mechanism can model benign traffic without any malicious samples.

As we known, the HTTP protocol use transfer metadata that provides the receiver to get some useful information from the headers. For example, the metadata can tell the web server which format is the best for the client's web browser or which file type is the client expects to receive.  And there is some common field of HTTP headers, such as User-Agent, Host, Referrer. The User-Agent describe the detail of the software application, it can make server respond the compatibility information to client's application. The Host field describes the domain or IP address for the requested resource. And the Referer field identifies the address of the webpage that linked to the resource being requested. According to the McAfee*5 reports, some malware starts to spoof their headers to avoid the detection.  These type of malware easily modify the metadata of headers to evade the detection system which is based on the information of headers. 
